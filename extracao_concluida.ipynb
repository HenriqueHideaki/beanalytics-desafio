{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução do desafio técnico BeAnalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Extração Automatizada de Dados do SteamDB\n",
    "Este notebook tem como objetivo extrair dados de pico de jogadores do site SteamDB, processá-los e armazená-los em um formato estruturado. Os dados extraídos foram carregados em serviços como Google BigQuery e Google Sheets.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instalação e Importação de Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urls e cabeçalhos das requisições que serão utilizadas para extrair os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = {\n",
    "    'gameratings': 'https://steamdb.info/stats/gameratings/',\n",
    "    'globaltopsellers': 'https://steamdb.info/stats/globaltopsellers/'\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': (\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "        '(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0 (Edition std-1)'\n",
    "    ),\n",
    "    'Accept': (\n",
    "        'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,'\n",
    "        'image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "    ),\n",
    "    'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Referer': 'https://steamdb.info/charts/',\n",
    "    'Sec-Ch-Ua': '\"Opera GX\";v=\"109\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',\n",
    "    'Sec-Ch-Ua-Arch': '\"x86\"',\n",
    "    'Sec-Ch-Ua-Bitness': '\"64\"',\n",
    "    'Sec-Ch-Ua-Full-Version': '\"109.0.5097.93\"',\n",
    "    'Sec-Ch-Ua-Full-Version-List': (\n",
    "        '\"Opera GX\";v=\"109.0.5097.93\", \"Not:A-Brand\";v=\"8.0.0.0\", \"Chromium\";v=\"123.0.6312.124\"'\n",
    "    ),\n",
    "    'Sec-Ch-Ua-Mobile': '?0',\n",
    "    'Sec-Ch-Ua-Model': '\"\"',\n",
    "    'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "    'Sec-Ch-Ua-Platform-Version': '\"15.0.0\"',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "COOKIES = {\n",
    "    'SLG_G_WPT_TO': 'pt',\n",
    "    'SLG_GWPT_Show_Hide_tmp': '1',\n",
    "    'SLG_wptGlobTipTmp': '1',\n",
    "    '__Host-cc': 'us',\n",
    "    'cf_clearance': (\n",
    "        'Fol6iBezomkbMSDRxzEGcntqaxMi49JrtuocqohAzus-1716068823-1.0.1.1-EwAqAA6pNgJzu8WXg2VzkuD0oMBj3S1JC5ub7Qg8VdS2UEvv'\n",
    "        'I8nYOIVtV.5KFeSxjMVR80tTwyZ9mtdcZGYoBg'\n",
    "    ),\n",
    "    '__cf_bm': (\n",
    "        'v8Uz9K829LAN3r9ACs35Y.r4xgq6MbiXzftfmptWH6I-1716075030-1.0.1.1-P2PT821NWNcgw7KOaIH88lj6K_1xEwXJbLFIKY4iNPCGKXzm'\n",
    "        'dQBYu2NWSBpEwC2SfYMEGU6tXD1R4ZlZbH0kDw'\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_CODE = 'br'\n",
    "CSV_FILE_PATH = 'dados/combined_price_history.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extração dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to dados/extracted_game_ratings_5.csv successfully.\n",
      "Data saved to dados/globaltopsellers_extracted_5.csv successfully.\n",
      "Data saved to dados/picos_historicos_5.csv successfully.\n",
      "Dados salvos em 'dados/picos_historicos.csv' com sucesso.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "Data saved to dados/combined_price_history.csv successfully.\n",
      "All data processed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def fetch_html(url, headers, cookies):\n",
    "    \"\"\"Fetch the HTML content of a webpage.\"\"\"\n",
    "    response = requests.get(url, headers=headers, cookies=cookies)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "    return None\n",
    "\n",
    "def parse_gameratings(html_content):\n",
    "    \"\"\"Parse HTML content and extract game ratings.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    pattern = re.compile(\n",
    "        r'<tr class=\"app\" data-appid=\"(\\d+)\" data-cache=\"[^\"]+\">.*?<td>\\s*(\\d+)\\.\\s*</td>.*?<td class=\"applogo\">.*?<a aria-hidden=\"true\" href=\"([^\"]+)\">.*?<td>.*?<a href=\"[^\"]+\">\\s*(.*?)\\s*</a>.*?<td class=\"text-right\">\\s*([\\d,]+)\\s*</td>.*?<td class=\"text-right\">\\s*([\\d,]+)\\s*</td>.*?<td class=\"text-right\">\\s*([\\d,]+)\\s*</td>.*?<td class=\"text-right b\">\\s*([\\d.]+)%\\s*</td>',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    matches = pattern.findall(str(soup))\n",
    "    data = [\n",
    "        {\n",
    "            'App ID': match[0],\n",
    "            'Posição': match[1],\n",
    "            'Link': 'https://steamdb.info' + match[2],\n",
    "            'Nome do Jogo': match[3],\n",
    "            'Avaliações Positivas': match[4].replace(',', ''),\n",
    "            'Avaliações Negativas': match[5].replace(',', ''),\n",
    "            'Avaliações Totais': match[6].replace(',', ''),\n",
    "            'Classificação': match[7]\n",
    "        }\n",
    "        for match in matches\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_globaltopsellers(html_content):\n",
    "    \"\"\"Parse HTML content and extract top sellers data.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'<tr class=\"app\" data-appid=\"(\\d+)\" data-cache=\"\\d+\">\\s*<td data-sort=\"\\d+\">#(\\d+)</td>\\s*<td class=\"applogo text-left\">'\n",
    "        r'<a href=\"/app/\\d+/charts/\" aria-hidden=\"true\"><img src=\"/static/img/applogo.svg\" alt=\"\"></a></td>\\s*<td>'\n",
    "        r'<a href=\"/app/\\d+/charts/\">(.*?)</a></td>\\s*<td data-sort=\"\"></td>\\s*<td data-sort=\"\\d+\">(.*?)</td>\\s*'\n",
    "        r'<td data-sort=\"\\d+\">(.*?)</td>\\s*</tr>',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    matches = pattern.findall(html_content.decode('utf-8'))\n",
    "    data = [\n",
    "        {\n",
    "            'App ID': appid,\n",
    "            'Posição': rank,\n",
    "            'Nome': name.strip(),\n",
    "            'Avaliações Positivas': positive.strip().replace(',', ''),\n",
    "            'Avaliações Negativas': negative.strip().replace(',', '')\n",
    "        }\n",
    "        for appid, rank, name, positive, negative in matches\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_charts_24h(html_content):\n",
    "    \"\"\"Parse HTML content and extract 24-hour peak data.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'<tr class=\"app\" data-appid=\"(\\d+)\"[^>]*>.*?'\n",
    "        r'<td class=\"applogo text-left\"><a href=\"[^\"]+\"><img[^>]+></a></td>.*?'\n",
    "        r'<td><a href=\"[^\"]+\">(.*?)</a></td>.*?'\n",
    "        r'<td data-sort=\"(\\d+)\">.*?</td>.*?'\n",
    "        r'<td data-sort=\"(\\d+)\">.*?</td>.*?'\n",
    "        r'<td data-sort=\"(\\d+)\">.*?</td>',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    matches = pattern.findall(html_content.decode('utf-8'))\n",
    "    data = [\n",
    "        {\n",
    "            'App ID': match[0],\n",
    "            'Nome do Jogo': match[1],\n",
    "            'Atual': match[2],\n",
    "            'Pico 24hrs': match[3],\n",
    "            'Pico de todos os tempos': match[4]\n",
    "        }\n",
    "        for match in matches\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def fetch_price_history(app_id, max_retries=5):\n",
    "    \"\"\"Fetch price history for a given app ID.\"\"\"\n",
    "    url = f'https://steamdb.info/api/GetPriceHistory/?appid={app_id}&cc={COUNTRY_CODE}'\n",
    "    HEADERS['Referer'] = f'https://steamdb.info/app/{app_id}/'\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, headers=HEADERS, cookies=COOKIES)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Processar os dados para o formato desejado\n",
    "            price_history = {\n",
    "                datetime.utcfromtimestamp(entry['x'] / 1000).strftime('%Y-%m-%d'): entry['y']\n",
    "                for entry in data['data']['history']\n",
    "            }\n",
    "\n",
    "            return price_history\n",
    "        \n",
    "        elif response.status_code in {429, 500, 502, 503, 504}:\n",
    "            time.sleep(2 ** attempt)  # Exponencial backoff\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    print(f\"Excedido o número máximo de tentativas para o App ID {app_id}.\")\n",
    "    return None\n",
    "\n",
    "def save_to_csv(df, file_path):\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Data saved to {file_path} successfully.')\n",
    "    \n",
    "def main():\n",
    "    # Carregar App IDs do arquivo picos_historicos.csv\n",
    "    picos = pd.read_csv(\"dados/picos_historicos.csv\")\n",
    "    app_ids = picos['App ID'][:10]  # Pegar os primeiros 10 app IDs\n",
    "\n",
    "    # Carregar dados existentes de histórico de preços\n",
    "    if os.path.exists(CSV_FILE_PATH):\n",
    "        df_existing = pd.read_csv(CSV_FILE_PATH)\n",
    "        price_histories = df_existing.to_dict('records')\n",
    "    else:\n",
    "        price_histories = []\n",
    "\n",
    "    # Buscar HTML para cada URL e salvar os dados extraídos\n",
    "    for key, url in URLS.items():\n",
    "        html_content = fetch_html(url, HEADERS, COOKIES)\n",
    "        if html_content:\n",
    "            if key == 'gameratings':\n",
    "                df = parse_gameratings(html_content)\n",
    "                save_to_csv(df, 'dados/extracted_game_ratings_5.csv')\n",
    "            elif key == 'globaltopsellers':\n",
    "                df = parse_globaltopsellers(html_content)\n",
    "                save_to_csv(df, 'dados/globaltopsellers_extracted_5.csv')\n",
    "            elif key == 'charts_24h':\n",
    "                df = parse_charts_24h(html_content)\n",
    "                save_to_csv(df, 'dados/picos_historicos_5.csv')\n",
    "                print(\"Dados salvos em 'dados/picos_historicos.csv' com sucesso.\")\n",
    "\n",
    "    # Buscar histórico de preços para cada App ID\n",
    "    for app_id in app_ids:\n",
    "        price_history = fetch_price_history(app_id)\n",
    "        if price_history:\n",
    "            price_histories.append({\n",
    "                'App ID': app_id,\n",
    "                'price_history': json.dumps(price_history)  # Converter o dicionário para string JSON\n",
    "            })\n",
    "            save_to_csv(pd.DataFrame(price_histories), CSV_FILE_PATH)  # Salvar após cada iteração\n",
    "            time.sleep(1)  # Esperar 1 segundo entre as requisições para evitar taxa de limite\n",
    "\n",
    "    print(\"All data processed and saved successfully.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enviando os dados ao Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para corrigir problemas comuns no CSV\n",
    "def clean_csv(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Verificar e corrigir problemas de aspas\n",
    "    cleaned_rows = []\n",
    "    for row in rows:\n",
    "        cleaned_row = [value.replace('\"', '').replace('\\n', ' ').strip() for value in row]\n",
    "        cleaned_rows.append(cleaned_row)\n",
    "\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(cleaned_rows)\n",
    "\n",
    "    print(f'CSV cleaned: {file_path}')\n",
    "\n",
    "# Função para carregar dados de um CSV para o BigQuery\n",
    "def upload_to_bigquery(csv_file_path, table_id):\n",
    "    credential_path = \"big_query/credentials.json\"\n",
    "\n",
    "# Carregar as credenciais do arquivo JSON\n",
    "    credentials = service_account.Credentials.from_service_account_file(credential_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "    # Limpar o CSV antes de carregar\n",
    "    clean_csv(csv_file_path)\n",
    "\n",
    "    # Carregar os dados do CSV para um DataFrame\n",
    "    df = pd.read_csv(csv_file_path, dtype=str)\n",
    "\n",
    "    # Configurar o job de carregamento\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        autodetect=True,\n",
    "    )\n",
    "\n",
    "    # Carregar os dados do DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()  # Esperar a conclusão do job\n",
    "\n",
    "    # Verificar se a tabela foi carregada com sucesso\n",
    "    destination_table = client.get_table(table_id)\n",
    "    print(f\"Carregado {destination_table.num_rows} linhas na tabela {table_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos de dados no projeto:\n",
      " - jogos\n",
      "CSV cleaned: dados/extracted_game_ratings.csv\n",
      "Carregado 250 linhas na tabela desafio-beanalytics.jogos.game_ratings.\n",
      "CSV cleaned: dados/globaltopsellers_extracted.csv\n",
      "Carregado 984 linhas na tabela desafio-beanalytics.jogos.topsellers.\n",
      "CSV cleaned: dados/picos_historicos.csv\n",
      "Carregado 7342 linhas na tabela desafio-beanalytics.jogos.picos_historicos.\n",
      "CSV cleaned: dados/combined_price_history.csv\n",
      "Carregado 193 linhas na tabela desafio-beanalytics.jogos.price_histories.\n"
     ]
    }
   ],
   "source": [
    "# Definir o caminho para o arquivo de credenciais JSON\n",
    "credential_path = \"big_query/credentials.json\"\n",
    "\n",
    "# Carregar as credenciais do arquivo JSON\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_path)\n",
    "\n",
    "# Criar um cliente do BigQuery\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Testar a conexão listando conjuntos de dados no projeto\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"Conjuntos de dados no projeto:\")\n",
    "    for dataset in datasets:\n",
    "        print(f\" - {dataset.dataset_id}\")\n",
    "else:\n",
    "    print(\"Nenhum conjunto de dados encontrado no projeto.\")\n",
    "\n",
    "\n",
    "# Definir os IDs do projeto, conjunto de dados e tabelas\n",
    "project_id = 'desafio-beanalytics'\n",
    "dataset_id = 'jogos'\n",
    "\n",
    "# Tabelas e arquivos CSV correspondentes\n",
    "tables_and_files = {\n",
    "    'game_ratings': 'dados/extracted_game_ratings.csv',\n",
    "    'topsellers': 'dados/globaltopsellers_extracted.csv',\n",
    "    'picos_historicos': 'dados/picos_historicos.csv',\n",
    "    'price_histories': 'dados/combined_price_history.csv'\n",
    "}\n",
    "\n",
    "# Carregar cada tabela para o BigQuery\n",
    "for table_name, csv_file_path in tables_and_files.items():\n",
    "    full_table_id = f'{project_id}.{dataset_id}.{table_name}'\n",
    "    upload_to_bigquery(csv_file_path, full_table_id)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
